{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 模型的核心思想是自注意力机制（self-attention）\n",
    "能注意输入序列的不同位置以计算该序列的表示的能力。Transformer 创建了多层自注意力层（self-attetion layers）组成的堆栈，下文的按比缩放的点积注意力（Scaled dot product attention）和多头注意力（Multi-head attention）部分对此进行了说明。\n",
    "\n",
    "一个 transformer 模型用自注意力层而非 RNNs 或 CNNs 来处理变长的输入。\n",
    "`这种通用架构有一系列的优势：`\n",
    "\n",
    "1、它不对数据间的时间/空间关系做任何假设。这是处理一组对象（objects）的理想选择（例如，星际争霸单位（StarCraft units））。\n",
    "2、层输出可以并行计算，而非像 RNN 这样的序列计算。\n",
    "3、远距离项可以影响彼此的输出，而无需经过许多 RNN 步骤或卷积层（例如，参见场景记忆 Transformer（Scene Memory Transformer））\n",
    "它能学习长距离的依赖。在许多序列任务中，这是一项挑战。\n",
    "\n",
    "`该架构的缺点是：`\n",
    "\n",
    "1、对于时间序列，一个单位时间的输出是从整个历史记录计算的，而非仅从输入和当前的隐含状态计算得到。这可能效率较低。\n",
    "2、如果输入确实有时间/空间的关系，像文本，则必须加入一些`位置编码`，否则模型将有效地看到一堆单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重点\n",
    "1、位置向量编码的含义\n",
    "\n",
    "2、和seq2seq的区别\n",
    "\n",
    "3、Q,K,V的设计（QKV图形）\n",
    "\n",
    "4、从attention谈transformer本质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下代码用来实现经典的编码器－解码器代码，并使用 cmn_eng　2.2w条中英文翻译数据，作为实例\n",
    "如果希望更详细的解读，强烈推荐阅读：https://zhuanlan.zhihu.com/p/28054589\n",
    "https://leovan.me/cn/2018/10/seq2seq-and-attention-machanism/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: sklearn in /home/zero/anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/zero/anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 修复中文显示问题\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# 判断是否gpu可用,如果可用设置gpu使用显存\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Almost half of the land has gone through desertification. In Dunhuang, no matter how many trees the locals grow, they eventually die off because of extreme drought.\\t甘肃近一半的土地遭受沙漠化侵害。在敦煌，由于当地十分干旱，所以无论当地百姓种多少树，最终都会枯死。\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/ch_en.txt', 'r', encoding='utf-8') as f:\n",
    "    contexts = f.readlines()\n",
    "contexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_pro(contexts):\n",
    "    processed_contexts_en = []\n",
    "    processed_contexts_ch = []\n",
    "    for line in contexts:\n",
    "        en, ch = re.split(r'\\t', line.strip(), 1)\n",
    "        # 如果句子过长，则跳过此句\n",
    "#         if len(ch) > 100:\n",
    "#             continue\n",
    "#         print(en, ch)\n",
    "        en = re.sub(r'([\\?\\.\\!\\,¿])', r' \\1', en)\n",
    "#         print(en)\n",
    "        en = re.sub(r'\\s+', ' ', en)\n",
    "        ch = re.sub(r'\\s+', ' ', ch)\n",
    "        en = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿]+', r' ', en)\n",
    "        ch = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿\\u4e00-\\u9fa5]+', r' ', ch)\n",
    "        en = '<start> ' + en[:200] + ' <end>'\n",
    "        ch = '<start> ' + ' '.join([i for i in ch[:100]]) + ' <end>'\n",
    "        \n",
    "        processed_contexts_en.append(en)\n",
    "        processed_contexts_ch.append(ch)\n",
    "    \n",
    "    return processed_contexts_en, processed_contexts_ch\n",
    "\n",
    "\n",
    "processed_contexts_en, processed_contexts_ch = data_pro(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(30000,\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # 创建清理过的输入输出对\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(processed_contexts_en)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(processed_contexts_ch)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_lang词典大小 53493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[3, 24, 101, 196, 253, 4], [24, 101]], '的')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
    "input_tensor[-1], target_tensor[-1]\n",
    "print('inp_lang词典大小', len(inp_lang.index_word))\n",
    "inp_lang.texts_to_sequences(['<start> hello world <end>', 'head of'])\n",
    "targ_lang.texts_to_sequences(['<start> 你 好 世 界 <end>', '你 好']), targ_lang.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000 85000 15001 15001\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.15)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "3 ----> <start>\n",
      "1 ----> the\n",
      "952 ----> picture\n",
      "12 ----> that\n",
      "2440 ----> emerged\n",
      "5 ----> ,\n",
      "7 ----> of\n",
      "8 ----> a\n",
      "151 ----> country\n",
      "12 ----> that\n",
      "26 ----> was\n",
      "1321 ----> changing\n",
      "113 ----> very\n",
      "807 ----> fast\n",
      "5 ----> ,\n",
      "1729 ----> surprised\n",
      "98 ----> even\n",
      "1 ----> the\n",
      "1281 ----> trend\n",
      "9256 ----> watchers\n",
      "2 ----> .\n",
      "4 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "3 ----> <start>\n",
      "16 ----> 国\n",
      "41 ----> 家\n",
      "10 ----> 这\n",
      "103 ----> 样\n",
      "1 ----> 的\n",
      "399 ----> 快\n",
      "526 ----> 速\n",
      "213 ----> 变\n",
      "236 ----> 化\n",
      "2 ----> ，\n",
      "157 ----> 使\n",
      "59 ----> 得\n",
      "79 ----> 那\n",
      "40 ----> 些\n",
      "63 ----> 最\n",
      "1341 ----> 敏\n",
      "2052 ----> 锐\n",
      "1 ----> 的\n",
      "369 ----> 观\n",
      "681 ----> 察\n",
      "120 ----> 着\n",
      "62 ----> 都\n",
      "185 ----> 感\n",
      "27 ----> 到\n",
      "3058 ----> 诧\n",
      "907 ----> 异\n",
      "5 ----> 。\n",
      "4 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 52]), TensorShape([256, 61]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self-attention机制的transformer实现:\n",
    "1、编码器:是由 N=6 个相同的网络层构成，每层中包含两个子层。第一层为一个 Multi-Head Self-Attention 层，第二层为一个 Position-Wise 全链接的前馈神经网络。每一层再应用一个残差连接 (Residual Connection) 7 和一个层标准化 (Layer Normalization) 8。则每一层的输出为 Layer(x + sublayer(x))，其中`sublayer(x)`为子层本身的函数实现。为了实现 为了实现残差连接，模型中所有的子层包括 Embedding 层的输出维度均为 dmodel=512。\n",
    "\n",
    "２、解码器: 也是由 N=6 个相同的网络层构成，但每层中包含三个子层，增加的第三层用于处理编码器的输出。同编码器一样，每一层应用一个残差连接和一个层标准化。除此之外，解码器对 Self-Attention 层进行了修改，确保对于位置  的预测仅依赖于位置在  之前的输出。"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重点的几个部分：\n",
    "① self_attention层(Multi-head attention)组件, Attention(Q, K, V) = softmax({Q . trans(K) /sqrt(d_k)) V\n",
    "\n",
    "② feed_forword层组件\n",
    "如图：\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    计算sequence的mask\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义　MultiHeadAttention:\n",
    "# 设置组件\n",
    "\n",
    "def MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).\n",
    "        pass\n",
    "    \n",
    "    def call(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        pass\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本编码器采用 Bahdanau 注意力\n",
    "p(y_i | y_1,..., y_{i-1}, x) = g(y_{i-1}, s_i, c_i)\n",
    "\n",
    "s_i = f (s_{i-1}, y_{i-1}, c_i)    # 即 s_i与 s_{i-1} , y_{i-1}, c_i 有关, c_i 是 attention 计算得来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_sz = batch_sz\n",
    "        # 返回隐状态 和 整个输出序列\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x形状为 (batch_size, seq_len)\n",
    "        # hidden 为初始化向量,形状为 (batch_size, units_dim)\n",
    "        em = self.embedding_dim(x)\n",
    "        output, state = self.gru(em, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 初始化一个样本输入隐向量\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "print(sample_hidden)\n",
    "\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('input shape: (batch size, sequence length) {}'.format(example_input_batch.shape))\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Ws = tf.keras.layers.Dense(units)\n",
    "        self.Wh = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        # 这里根据使用方法，query 是 Si中的一个\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        # eij = v.tanh(w.s + w.h) 这个式子比较特别，需要使用广播机制\n",
    "        # w . S -> （batch_size, 1, units）, W . h -> (batch_size, seq_len, units)满足广播机制，且 score —> (batch_size, seq_len, 1)\n",
    "        score = self.V(tf.tanh(self.Ws(query) + self.Wh(values)))\n",
    "        # 计算 权重 attention_weight 形状 (batch_size, seq_len, 1)，在 seqence 方向上 转换概率值，所以 axis = 1\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # 计算 content_vector,沿着 axis=1的方向求和\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention2, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 注意这里使用了 build 方法,其主要用于根据input_shape创建 layer的Variable\n",
    "        self.Ws = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.Wh = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(input_shape[-1],1),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        score = (tf.tanh(query @ self.Ws + values @ self.Wh)) @ self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "\n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "print(attention_weights[0])  # 可以看出，没有经过训练的网络，权重参数是比较均衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder 输入和输出是什么？ decoder是逐个解码，所以，输入是 x -> (batch_size, 1), decoder侧的隐向量 hidden -> (batch_size, dec_units)\n",
    "# 输出是 下一个状态的隐向量 state 及 output,以及为了方便打印的 attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size    # 词表大小\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)    # 词嵌入维度\n",
    "        self.dec_units = dec_units    # 解码器 gru的矩阵的维度，即 hidden_size大小\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, \n",
    "                                       return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # 逐个计算 x,hidden,enc_output计算得到的权重计算后的 context_vec 和 attention_weight\n",
    "        context_vec, attention_weights = self.attention(hidden, enc_output)\n",
    "        # x的维度是 batch_size * 1，经过 embedding 后的维度为 (batch_sz, 1, embedding_size)\n",
    "        x = self.embedding(x)\n",
    "        # 拼接 x 和 context_vec，得到 (batch_size, 1, hidden_size+embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vec, 1), x], axis=-1)\n",
    "        # 然后将 x 输入到 gru 中\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        # # dense输入可以为2维,也可以为3维，dense会自动在 time_step维度上展开,所以考虑输出后的维度为2维，先进行维度转换\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # 输出形状 (batch_size, vocab_size)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单测试\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 计算mask\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# loss的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "tf.train.Checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    # encoder的最后输出隐状态作为 decoder 初始输入隐状态\n",
    "    dec_hidden = enc_hidden\n",
    "    # 使用 <start> 做初始解码输入\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # 教师强制 - 将目标词作为下一个输入\n",
    "    for t in range(1, targ.shape[1]):    # 逐个输入\n",
    "      # 将编码器输出 （enc_output） 传送至解码器\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # 使用教师强制，预测阶段正常解码应该用 predictions 作为下一个 time_step 的输入\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "  # 平均 batch_loss\n",
    "  batch_loss = (loss / int(targ.shape[0]))\n",
    "  # 总的可训练参数:\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻译\n",
    "评估函数类似于训练循环，不同之处在于在这里我们不使用 `教师强制`。每个时间步的解码器输入是其先前的预测、隐藏层状态和编码器输出。\n",
    "当模型预测 结束标记 时停止预测。\n",
    "存储 每个时间步的注意力权重。\n",
    "请注意：对于一个输入，编码器输出仅计算一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    inputs = inp_lang.texts_to_sequences([sentence])\n",
    "    print('inputs', inputs)\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 存储注意力权重以便后面制图\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 预测的 ID 被输送回模型\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力权重制图函数\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恢复最新的检查点并验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复检查点目录 （checkpoint_dir） 中最新的检查点\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate('<start> spreading the cost of their work over all users . <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('<start> The House of Representatives approved a similar package earlier this month ! <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('<start> Many companies seek reassurance that all is well by installing cyber-security tools to monitor employees ! <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2-1)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
