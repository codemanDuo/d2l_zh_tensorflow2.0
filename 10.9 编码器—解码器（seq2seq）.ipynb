{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下代码用来实现经典的编码器－解码器代码，并使用 cmn_eng　2.2w条中英文翻译数据，作为实例\n",
    "如果希望更详细的解读，强烈推荐阅读：https://zhuanlan.zhihu.com/p/28054589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\r\n",
      "Requirement already satisfied: sklearn in /home/zero/anaconda3/lib/python3.6/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/zero/anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Got it?\\t你懂了吗？\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/cmn-eng.txt', 'r', encoding='utf-8') as f:\n",
    "    contexts = f.readlines()\n",
    "contexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6927\n",
      "3424\n",
      "<start> Got it ? <end>\n",
      "<start> 你 懂 了 吗 ？ <end>\n",
      "max_len_ch:  51\n",
      "max_len_en:  38\n"
     ]
    }
   ],
   "source": [
    "def data_pro(contexts):\n",
    "    processed_contexts_en = []\n",
    "    processed_contexts_ch = []\n",
    "    for line in contexts:\n",
    "        en, ch = re.split(r'\\t', line.strip())\n",
    "#         print(en, ch)\n",
    "        en = re.sub(r'([\\?\\.\\!\\,¿])', r' \\1', en)\n",
    "#         print(en)\n",
    "        en = re.sub(r'\\s+', ' ', en)\n",
    "        ch = re.sub(r'\\s+', ' ', ch)\n",
    "        en = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿]+', r' ', en)\n",
    "        ch = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿\\u4e00-\\u9fa5]+', r' ', ch)\n",
    "        en = '<start> ' + en + ' <end>'\n",
    "        ch = '<start> ' + ' '.join([i for i in ch]) + ' <end>'\n",
    "        \n",
    "        processed_contexts_en.append(en)\n",
    "        processed_contexts_ch.append(ch)\n",
    "    \n",
    "    return processed_contexts_en, processed_contexts_ch\n",
    "\n",
    "max_len_en = set()\n",
    "max_len_ch = set()\n",
    "\n",
    "def get_vocab(processed_contexts_en, processed_contexts_ch):\n",
    "    en_vocab = set()\n",
    "    ch_vocab = set()\n",
    "    for en, ch in zip(processed_contexts_en, processed_contexts_ch):\n",
    "        len_en = len([w for w in re.split(r'\\s', en)])\n",
    "        len_ch = len([w for w in re.split(r'\\s', ch)])\n",
    "        max_len_en.add(len_en)\n",
    "        max_len_ch.add(len_ch)\n",
    "        \n",
    "        en_vocab.update(set([w for w in re.split(r'\\s', en)]))\n",
    "        ch_vocab.update(set([w for w in re.split(r'\\s', ch)]))\n",
    "\n",
    "    return en_vocab, ch_vocab\n",
    "\n",
    "\n",
    "processed_contexts_en, processed_contexts_ch = data_pro(contexts)\n",
    "en_vocab, ch_vocab = get_vocab(processed_contexts_en, processed_contexts_ch)\n",
    "print(len(en_vocab))\n",
    "print(len(ch_vocab))\n",
    "print(processed_contexts_en[10])\n",
    "print(processed_contexts_ch[10])\n",
    "\n",
    "print('max_len_ch: ', max(max_len_ch))\n",
    "print('max_len_en: ', max(max_len_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # 创建清理过的输入输出对\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(processed_contexts_en)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(processed_contexts_ch)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 7, 34, 352, 515, 2], [7, 34]], '<start>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
    "input_tensor[-1], target_tensor[-1]\n",
    "inp_lang.texts_to_sequences(['<start> hello world <end>', 'head of'])\n",
    "targ_lang.texts_to_sequences(['<start> 你 好 世 界 <end>', '你 好']), targ_lang.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18763 18763 3312 3312\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.15)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "66 ----> ve\n",
      "230 ----> heard\n",
      "23 ----> this\n",
      "420 ----> story\n",
      "166 ----> before\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> 我\n",
      "55 ----> 以\n",
      "103 ----> 前\n",
      "280 ----> 听\n",
      "131 ----> 过\n",
      "25 ----> 这\n",
      "32 ----> 个\n",
      "351 ----> 故\n",
      "64 ----> 事\n",
      "3 ----> 。\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 38]), TensorShape([64, 46]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本编码器采用 Bahdanau 注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_sz = batch_sz\n",
    "        # 返回隐状态 和 整个输出序列\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x形状为 (batch_size, seq_len)\n",
    "        # hidden 为初始化向量,形状为 (batch_size, units_dim)\n",
    "        em = self.embedding_dim(x)\n",
    "        output, state = self.gru(em, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(64, 1024), dtype=float32)\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 38, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 初始化一个样本输入\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "print(sample_hidden)\n",
    "\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 注意这里使用了 build 方法,其主要用于根据input_shape创建 layer的Variable\n",
    "        self.Ws = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.Wh = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(input_shape[-1],1),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        a_score = self.V(self.Ws)\n",
    "        context_vec = 1\n",
    "        attention_weights = 1\n",
    "    \n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        self.Ws = tf.keras.layers.Dense(units)\n",
    "        self.Wh = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        # 这里根据使用方法，query 是 Ci中的一个\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        attention_weights = 1\n",
    "    \n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def call(self):\n",
    "        passs\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2-1)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
