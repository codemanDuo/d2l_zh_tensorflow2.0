{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下代码用来实现经典的编码器－解码器代码，并使用 cmn_eng　2.2w条中英文翻译数据，作为实例\n",
    "如果希望更详细的解读，强烈推荐阅读：https://zhuanlan.zhihu.com/p/28054589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\r\n",
      "Requirement already satisfied: sklearn in /home/zero/anaconda3/lib/python3.6/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/zero/anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# 判断是否gpu可用,如果可用设置gpu使用显存\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Got it?\\t你懂了吗？\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/cmn-eng.txt', 'r', encoding='utf-8') as f:\n",
    "    contexts = f.readlines()\n",
    "contexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_pro(contexts):\n",
    "    processed_contexts_en = []\n",
    "    processed_contexts_ch = []\n",
    "    for line in contexts:\n",
    "        en, ch = re.split(r'\\t', line.strip())\n",
    "#         print(en, ch)\n",
    "        en = re.sub(r'([\\?\\.\\!\\,¿])', r' \\1', en)\n",
    "#         print(en)\n",
    "        en = re.sub(r'\\s+', ' ', en)\n",
    "        ch = re.sub(r'\\s+', ' ', ch)\n",
    "        en = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿]+', r' ', en)\n",
    "        ch = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿\\u4e00-\\u9fa5]+', r' ', ch)\n",
    "        en = '<start> ' + en + ' <end>'\n",
    "        ch = '<start> ' + ' '.join([i for i in ch]) + ' <end>'\n",
    "        \n",
    "        processed_contexts_en.append(en)\n",
    "        processed_contexts_ch.append(ch)\n",
    "    \n",
    "    return processed_contexts_en, processed_contexts_ch\n",
    "\n",
    "\n",
    "processed_contexts_en, processed_contexts_ch = data_pro(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # 创建清理过的输入输出对\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(processed_contexts_en)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(processed_contexts_ch)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 7, 34, 352, 515, 2], [7, 34]], '<start>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
    "input_tensor[-1], target_tensor[-1]\n",
    "inp_lang.texts_to_sequences(['<start> hello world <end>', 'head of'])\n",
    "targ_lang.texts_to_sequences(['<start> 你 好 世 界 <end>', '你 好']), targ_lang.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18763 18763 3312 3312\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.15)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "23 ----> this\n",
      "294 ----> clock\n",
      "2081 ----> loses\n",
      "150 ----> three\n",
      "396 ----> minutes\n",
      "8 ----> a\n",
      "104 ----> day\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "25 ----> 这\n",
      "32 ----> 个\n",
      "733 ----> 钟\n",
      "139 ----> 每\n",
      "24 ----> 天\n",
      "563 ----> 慢\n",
      "184 ----> 三\n",
      "245 ----> 分\n",
      "733 ----> 钟\n",
      "3 ----> 。\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 38]), TensorShape([64, 46]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本编码器采用 Bahdanau 注意力\n",
    "p(y_i | y_1,..., y_{i-1}, x) = g(y_{i-1}, s_i, c_i)\n",
    "\n",
    "s_i = f (s_{i-1}, y_{i-1}, c_i)    # 即 s_i与 s_{i-1} , y_{i-1}, c_i 有关, c_i 是 attention 计算得来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_sz = batch_sz\n",
    "        # 返回隐状态 和 整个输出序列\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x形状为 (batch_size, seq_len)\n",
    "        # hidden 为初始化向量,形状为 (batch_size, units_dim)\n",
    "        em = self.embedding_dim(x)\n",
    "        output, state = self.gru(em, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(64, 1024), dtype=float32)\n",
      "input shape: (batch size, sequence length) (64, 38)\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 38, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 初始化一个样本输入隐向量\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "print(sample_hidden)\n",
    "\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('input shape: (batch size, sequence length) {}'.format(example_input_batch.shape))\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Ws = tf.keras.layers.Dense(units)\n",
    "        self.Wh = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        # 这里根据使用方法，query 是 Si中的一个\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        # eij = v.tanh(w.s + w.h) 这个式子比较特别，需要使用广播机制\n",
    "        # w . S -> （batch_size, 1, units）, W . h -> (batch_size, seq_len, units)满足广播机制，且 score —> (batch_size, seq_len, 1)\n",
    "        score = self.V(tf.tanh(self.Ws(query) + self.Wh(values)))\n",
    "        # 计算 权重 attention_weight 形状 (batch_size, seq_len, 1)，在 seqence 方向上 转换概率值，所以 axis = 1\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # 计算 content_vector,沿着 axis=1的方向求和\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention2, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 注意这里使用了 build 方法,其主要用于根据input_shape创建 layer的Variable\n",
    "        self.Ws = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.Wh = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(input_shape[-1],1),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        score = (tf.tanh(query @ self.Ws + values @ self.Wh)) @ self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "\n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 38, 1)\n",
      "tf.Tensor(\n",
      "[[0.02686527]\n",
      " [0.02679701]\n",
      " [0.0268653 ]\n",
      " [0.02677984]\n",
      " [0.02685693]\n",
      " [0.02661959]\n",
      " [0.02715518]\n",
      " [0.02726946]\n",
      " [0.02739378]\n",
      " [0.02676431]\n",
      " [0.02646108]\n",
      " [0.02629999]\n",
      " [0.0262069 ]\n",
      " [0.02615008]\n",
      " [0.02611453]\n",
      " [0.02609221]\n",
      " [0.02607832]\n",
      " [0.02606978]\n",
      " [0.02606461]\n",
      " [0.02606152]\n",
      " [0.0260597 ]\n",
      " [0.02605863]\n",
      " [0.026058  ]\n",
      " [0.02605763]\n",
      " [0.02605742]\n",
      " [0.0260573 ]\n",
      " [0.02605722]\n",
      " [0.02605718]\n",
      " [0.02605715]\n",
      " [0.02605713]\n",
      " [0.02605712]\n",
      " [0.02605712]\n",
      " [0.02605711]\n",
      " [0.02605711]\n",
      " [0.02605711]\n",
      " [0.0260571 ]\n",
      " [0.0260571 ]\n",
      " [0.0260571 ]], shape=(38, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "print(attention_weights[0])  # 可以看出，没有经过训练的网络，权重参数是比较均衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size    # 词表大小\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)    # 词嵌入维度\n",
    "        self.dec_units = dec_units    # 解码器 gru的矩阵的维度，即 hidden_size大小\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, \n",
    "                                       return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # 逐个计算 x,hidden,enc_output计算得到的权重计算后的 context_vec 和 attention_weight\n",
    "        context_vec, attention_weights = self.attention(hidden, enc_output)\n",
    "        # x的维度是 batch_size * 1，经过 embedding 后的维度为 (batch_sz, 1, embedding_size)\n",
    "        x = self.embedding(x)\n",
    "        # 拼接 x 和 context_vec，得到 (batch_size, 1, hidden_size+embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vec, 1), x], axis=-1)\n",
    "        # 然后将 x 输入到 gru 中\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        # # dense输入可以为2维,也可以为3维，dense会自动在 time_step维度上展开,所以考虑输出后的维度为2维，先进行维度转换\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # 输出形状 (batch_size, vocab_size)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 3407)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 计算mask\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# loss的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.Checkpoint at 0x7f08df834320>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "tf.train.Checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # 教师强制 - 将目标词作为下一个输入\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # 将编码器输出 （enc_output） 传送至解码器\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # 使用教师强制\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0940\n",
      "Epoch 1 Batch 100 Loss 1.1176\n",
      "Epoch 1 Batch 200 Loss 0.9049\n",
      "Epoch 1 Loss 1.0868\n",
      "Time taken for 1 epoch 155.85847187042236 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.9224\n",
      "Epoch 2 Batch 100 Loss 0.8504\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2-1)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
