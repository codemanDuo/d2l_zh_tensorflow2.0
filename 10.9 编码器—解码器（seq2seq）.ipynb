{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下代码用来实现经典的编码器－解码器代码，并使用 cmn_eng　2.2w条中英文翻译数据，作为实例\n",
    "如果希望更详细的解读，强烈推荐阅读：https://zhuanlan.zhihu.com/p/28054589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sklearn in c:\\users\\jack\\appdata\\local\\conda\\conda\\envs\\tf2\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jack\\appdata\\local\\conda\\conda\\envs\\tf2\\lib\\site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\jack\\appdata\\local\\conda\\conda\\envs\\tf2\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\jack\\appdata\\local\\conda\\conda\\envs\\tf2\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jack\\appdata\\local\\conda\\conda\\envs\\tf2\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Got it?\\t你懂了吗？\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/cmn-eng.txt', 'r', encoding='utf-8') as f:\n",
    "    contexts = f.readlines()\n",
    "contexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_pro(contexts):\n",
    "    processed_contexts_en = []\n",
    "    processed_contexts_ch = []\n",
    "    for line in contexts:\n",
    "        en, ch = re.split(r'\\t', line.strip())\n",
    "#         print(en, ch)\n",
    "        en = re.sub(r'([\\?\\.\\!\\,¿])', r' \\1', en)\n",
    "#         print(en)\n",
    "        en = re.sub(r'\\s+', ' ', en)\n",
    "        ch = re.sub(r'\\s+', ' ', ch)\n",
    "        en = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿]+', r' ', en)\n",
    "        ch = re.sub(r'[^a-zA-Z\\?\\.\\!\\,。？！，、¿\\u4e00-\\u9fa5]+', r' ', ch)\n",
    "        en = '<start> ' + en + ' <end>'\n",
    "        ch = '<start> ' + ' '.join([i for i in ch]) + ' <end>'\n",
    "        \n",
    "        processed_contexts_en.append(en)\n",
    "        processed_contexts_ch.append(ch)\n",
    "    \n",
    "    return processed_contexts_en, processed_contexts_ch\n",
    "\n",
    "\n",
    "processed_contexts_en, processed_contexts_ch = data_pro(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # 创建清理过的输入输出对\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(processed_contexts_en)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(processed_contexts_ch)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 7, 34, 352, 515, 2], [7, 34]], '<start>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
    "input_tensor[-1], target_tensor[-1]\n",
    "inp_lang.texts_to_sequences(['<start> hello world <end>', 'head of'])\n",
    "targ_lang.texts_to_sequences(['<start> 你 好 世 界 <end>', '你 好']), targ_lang.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18763 18763 3312 3312\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.15)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "40 ----> with\n",
      "5 ----> the\n",
      "12 ----> t\n",
      "6027 ----> .v\n",
      "3 ----> .\n",
      "34 ----> on\n",
      "22 ----> ,\n",
      "51 ----> how\n",
      "32 ----> can\n",
      "7 ----> you\n",
      "192 ----> keep\n",
      "31 ----> your\n",
      "337 ----> mind\n",
      "34 ----> on\n",
      "31 ----> your\n",
      "998 ----> studies\n",
      "9 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "7 ----> 你\n",
      "146 ----> 开\n",
      "157 ----> 着\n",
      "268 ----> 电\n",
      "513 ----> 视\n",
      "132 ----> 怎\n",
      "39 ----> 么\n",
      "36 ----> 能\n",
      "357 ----> 安\n",
      "153 ----> 心\n",
      "116 ----> 学\n",
      "426 ----> 习\n",
      "364 ----> 呢\n",
      "13 ----> ？\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 38]), TensorShape([64, 46]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本编码器采用 Bahdanau 注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_sz = batch_sz\n",
    "        # 返回隐状态 和 整个输出序列\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x形状为 (batch_size, seq_len)\n",
    "        # hidden 为初始化向量,形状为 (batch_size, units_dim)\n",
    "        em = self.embedding_dim(x)\n",
    "        output, state = self.gru(em, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(64, 1024), dtype=float32)\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 38, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 初始化一个样本输入\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "print(sample_hidden)\n",
    "\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Ws = tf.keras.layers.Dense(units)\n",
    "        self.Wh = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        # 这里根据使用方法，query 是 Si中的一个\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        # eij = v.tanh(w.s + w.h) 这个式子比较特别，需要使用广播机制\n",
    "        # w . S -> （batch_size, 1, units）, W . h -> (batch_size, seq_len, units)满足广播机制，且 score —> (batch_size, seq_len, 1)\n",
    "        score = self.V(tf.tanh(self.Ws(query) + self.Wh(values)))\n",
    "        # 计算 权重 attention_weight 形状 (batch_size, seq_len, 1)，在 seqence 方向上 转换概率值，所以 axis = 1\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # 计算 content_vector,沿着 axis=1的方向求和\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 实现的是输入两个向量，输出　上下文向量　和　权重，其中权重主要是为了后续 方便　可视化使用,那么，BahdanauAttention中：\n",
    "# Ci = reduce_sum(α * Hs)，　αij = exp(eij) / reduce_sum(exp(eij)), eij = v.tanh(w.s + w.h)\n",
    "class BahdanauAttention2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        # 初始化需要训练的参数 eij = v.tanh(w.s + w.h)，乘相当于dense操作\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention2, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 注意这里使用了 build 方法,其主要用于根据input_shape创建 layer的Variable\n",
    "        self.Ws = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.Wh = tf.Variable(tf.random.normal(shape=(input_shape[-1],units),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(input_shape[-1],1),stddev=0.01,mean=0,dtype=tf.float32))\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        输入: query, values,即S_t-1, hidden_ts\n",
    "        输出: context_vec, attention_weights\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        context_vec = 1\n",
    "        score = (tf.tanh(query @ self.Ws + values @ self.Wh)) @ self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vec = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "    \n",
    "        return context_vec, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 38, 1)\n",
      "tf.Tensor(\n",
      "[[0.02630804]\n",
      " [0.02631733]\n",
      " [0.02630732]\n",
      " [0.02630997]\n",
      " [0.02631881]\n",
      " [0.0263202 ]\n",
      " [0.02635599]\n",
      " [0.02635256]\n",
      " [0.02633931]\n",
      " [0.02631946]\n",
      " [0.02630846]\n",
      " [0.02630564]\n",
      " [0.02630621]\n",
      " [0.02630784]\n",
      " [0.02630951]\n",
      " [0.02631087]\n",
      " [0.02631188]\n",
      " [0.02631259]\n",
      " [0.02631307]\n",
      " [0.02631339]\n",
      " [0.02631361]\n",
      " [0.02631375]\n",
      " [0.02631384]\n",
      " [0.0263139 ]\n",
      " [0.02631395]\n",
      " [0.02631397]\n",
      " [0.02631399]\n",
      " [0.026314  ]\n",
      " [0.02631401]\n",
      " [0.02631401]\n",
      " [0.02631401]\n",
      " [0.02631402]\n",
      " [0.02631402]\n",
      " [0.02631402]\n",
      " [0.02631402]\n",
      " [0.02631402]\n",
      " [0.02631402]\n",
      " [0.02631402]], shape=(38, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention2(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "print(attention_weights[0])  # 可以看出，没有经过训练的网络，权重参数是比较均衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def call(self):\n",
    "        passs\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
