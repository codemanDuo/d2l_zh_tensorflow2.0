{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于inception层比较复杂，这里使用自定义一个层，这里使用tf.keras.models.Model的模型构建方法，更加灵活\n",
    "class Inception(tf.keras.layers.Layer):\n",
    "    def __init__(self,c1, c2, c3, c4):\n",
    "        super().__init__()\n",
    "        # 线路1，单1 x 1卷积层\n",
    "        self.p1_1 = tf.keras.layers.Conv2D(c1, kernel_size=1, activation='relu', padding='same')\n",
    "        # 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2_1 = tf.keras.layers.Conv2D(c2[0], kernel_size=1, padding='same', activation='relu')\n",
    "        self.p2_2 = tf.keras.layers.Conv2D(c2[1], kernel_size=3, padding='same',\n",
    "                              activation='relu')\n",
    "        # 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3_1 = tf.keras.layers.Conv2D(c3[0], kernel_size=1, padding='same', activation='relu')\n",
    "        self.p3_2 = tf.keras.layers.Conv2D(c3[1], kernel_size=5, padding='same',\n",
    "                              activation='relu')\n",
    "        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4_1 = tf.keras.layers.MaxPool2D(pool_size=3, padding='same', strides=1)\n",
    "        self.p4_2 = tf.keras.layers.Conv2D(c4, kernel_size=1, padding='same', activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        return tf.concat([p1, p2, p3, p4], axis=-1)  # 在通道维上连结输出,这里注意一下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Inception at 0x20fa5ee43c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inception(64, (96, 128), (16, 32), 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3×33×3最大池化层来减小输出高宽。第一模块使用一个64通道的7×77×7卷积层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = tf.keras.models.Sequential()\n",
    "b1.add(tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same', activation='relu'))\n",
    "b1.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二模块使用2个卷积层：首先是64通道的1×11×1卷积层，然后是将通道增大3倍的3×33×3卷积层。它对应Inception块中的第二条线路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = tf.keras.models.Sequential()\n",
    "b2.add(tf.keras.layers.Conv2D(64, kernel_size=1, padding='same', activation='relu'))\n",
    "b2.add(tf.keras.layers.Conv2D(192, kernel_size=3, padding='same', activation='relu'))\n",
    "b2.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为64+128+32+32=25664+128+32+32=256，其中4条线路的输出通道数比例为64:128:32:32=2:4:1:112832:32=241:164:128:32:32=2:4:1:1。其中第二、第三条线路先分别将输入通道数减小至96/192=1/296/192=1/2和16/192=1/1216/192=1/12后，再接上第二层卷积层。第二个Inception块输出通道数增至128+192+96+64=480128+192+96+64=480，每条线路的输出通道数之比为128:192:96:64=4:6:3:219296:64 = 463:2128:192:96:64=4:6:3:2。其中第二、第三条线路先分别将输入通道数减小至128/256=1/2128/256=1/2和32/256=1/832/256=1/8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = tf.keras.models.Sequential()\n",
    "b3.add(Inception(64, (96, 128), (16, 32), 32))\n",
    "b3.add(Inception(128, (128, 192), (32, 96), 64))\n",
    "b3.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192+208+48+64=512192+208+48+64=512、160+224+64+64=512160+224+64+64=512、128+256+64+64=512128+256+64+64=512、112+288+64+64=528112+288+64+64=528和256+320+128+128=832256+320+128+128=832。这些线路的通道数分配和第三模块中的类似，首先含3×33×3卷积层的第二条线路输出最多通道，其次是仅含1×11×1卷积层的第一条线路，之后是含5×55×5卷积层的第三条线路和含3×33×3最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = tf.keras.models.Sequential()\n",
    "b4.add(Inception(192, (96, 208), (16, 48), 64))\n",
    "b4.add(Inception(160, (112, 224), (24, 64), 64))\n",
    "b4.add(Inception(128, (128, 256), (24, 64), 64))\n",
    "b4.add(Inception(112, (144, 288), (32, 64), 64))\n",
    "b4.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "b4.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第五模块有输出通道数为256+320+128+128=832256+320+128+128=832和384+384+128+128=1024384+384+128+128=1024的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = tf.keras.models.Sequential()\n",
    "b5.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "b5.add(Inception(384, (192, 384), (48, 128), 128))\n",
    "b5.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "\n",
    "net = tf.keras.models.Sequential([b1, b2, b3, b4, b5, tf.keras.layers.Dense(10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。本节里我们将输入的高和宽从224降到96来简化计算。下面演示各个模块之间的输出的形状变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential output shape:\t (1, 24, 24, 64)\n",
      "sequential_1 output shape:\t (1, 12, 12, 192)\n",
      "sequential_2 output shape:\t (1, 6, 6, 480)\n",
      "sequential_3 output shape:\t (1, 3, 3, 832)\n",
      "sequential_4 output shape:\t (1, 1024)\n",
      "dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 96, 96, 1))\n",
    "for layer in net.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "def data_scale(x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = x / 255.0\n",
    "    x = tf.reshape(x, (x.shape[0], x.shape[1], 1))\n",
    "    x = tf.image.resize_with_pad(image=x, target_height=224,target_width=224)\n",
    "    \n",
    "    return x, y\n",
    "# 由于笔记本训练太慢了，使用1000条数据，跑一下先,算力够的可以直接使用全部数据更加明显\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train[0:5000],y_train[0:5000])).shuffle(20).map(data_scale).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test[0:1000],y_test[0:1000])).shuffle(20).map(data_scale).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器和损失函数\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-2)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "net.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38/157 [======>.......................] - ETA: 24:41 - loss: 2.6850 - accuracy: 0.0996"
     ]
    }
   ],
   "source": [
    "net.fit_generator(train_db, epochs=1, validation_data=test_db)    # 这里就不跑太多轮了，有机器可以自己调参跑个好的结果\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
