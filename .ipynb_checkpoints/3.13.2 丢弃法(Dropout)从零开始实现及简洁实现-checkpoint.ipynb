{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "def data_scale(x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = tf.squeeze(tf.reshape(x, shape=(-1, 1)))\n",
    "    x = x / 255.0\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    return x, y\n",
    "    \n",
    "    \n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(64).map(data_scale).batch(64)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(64).map(data_scale).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_out函数编写\n",
    "from tensorflow import keras, nn,losses\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "def dropout(X, drop_prob=0.1):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return tf.zeros_like(X)\n",
    "    \n",
    "    mask = tf.random.uniform(shape=X.shape, minval=0, maxval=1) < keep_prob   # 此处解释一下：使用0-1区间均匀分布的x.shape的矩阵元素小于 keepprob生成的的True or False矩阵\n",
    "    # 注意这里是数乘\n",
    "#     print(mask)\n",
    "    # 使用 0,1 矩阵数乘计算，然后使用 1/ keep_prob 尺度进行拉伸\n",
    "    #初始mask为一个bool型数组，故需要强制类型转换，这个和其他框架不一样\n",
    "    after_dropout = tf.cast(mask, dtype=tf.float32) * tf.cast(X, dtype=tf.float32) / keep_prob\n",
    "    return after_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]], shape=(2, 8), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=72, shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11., 12., 13., 14., 15.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(0, 16), shape=(2, 8))\n",
    "print(X)\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=86, shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 0.,  2.,  0.,  6.,  8., 10., 12., 14.],\n",
       "       [16., 18., 20., 22., 24., 26., 28., 30.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(2, 8), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "W1 = tf.Variable(tf.random.normal(stddev=0.1,shape=(784, 128)))\n",
    "B1 = tf.Variable(tf.random.normal(stddev=0.1, shape=(1,128)))\n",
    "W2 = tf.Variable(tf.random.normal(stddev=0.1,shape=(128, 64)))\n",
    "B2= tf.Variable(tf.random.normal(stddev=0.1, shape=(1,64)))\n",
    "W3 = tf.Variable(tf.random.normal(stddev=0.1,shape=(64, 10)))\n",
    "B3 = tf.Variable(tf.random.normal(stddev=0.1, shape=(1,10)))\n",
    "\n",
    "params = [W1, B1, W2, B2, W3, B3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def net(X, training=True, drop_rate=0.2):\n",
    "    X = tf.nn.relu(X @ W1 + B1)\n",
    "    if training:\n",
    "        X = dropout(X, drop_rate)\n",
    "    X = tf.nn.relu(X @ W2 + B2)\n",
    "    if training:\n",
    "        X = dropout(X, drop_rate)\n",
    "    X = X @ W3 + B3\n",
    "    return tf.math.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def cross_entropy(y, y_hat):\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    y = tf.one_hot(y,axis=-1,depth=10)\n",
    "    y = tf.cast(y,dtype=tf.float32)\n",
    "#     print(y[0], y_hat[0])\n",
    "    y_hat = tf.cast(tf.reshape(y_hat, shape=(y.shape)), dtype=tf.float32)\n",
    "    l = (-1) * y * tf.math.log(y_hat)# 这个会出现 log(0)导致 loss出现nan，所以在后面加一个小常数\n",
    "    l = (-1) * y * tf.math.log(y_hat + 1e-10)\n",
    "#     print(l)\n",
    "    loss = tf.reduce_sum(l) / y.shape[0]\n",
    "#     print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0, 0.0\n",
    "    for x, y in data_iter:\n",
    "        yhat = net(x)\n",
    "        acc_sum += tf.reduce_sum(tf.cast(tf.argmax(yhat, axis=1) == tf.cast(y, dtype=tf.int64), dtype=tf.float32))\n",
    "#         n += y.shape[0]\n",
    "#     return acc_sum / n  #这个更合理\n",
    "    return acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8172, shape=(), dtype=float32, numpy=763.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "origin_acc = evaluate_accuracy(test_db, net)\n",
    "origin_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0096, train acc 0.779, test acc 8289.000\n",
      "epoch 2, loss 0.0067, train acc 0.846, test acc 8355.000\n",
      "epoch 3, loss 0.0061, train acc 0.860, test acc 8472.000\n",
      "epoch 4, loss 0.0057, train acc 0.866, test acc 8555.000\n",
      "epoch 5, loss 0.0055, train acc 0.873, test acc 8521.000\n",
      "epoch 6, loss 0.0053, train acc 0.878, test acc 8593.000\n",
      "epoch 7, loss 0.0051, train acc 0.882, test acc 8647.000\n",
      "epoch 8, loss 0.0049, train acc 0.884, test acc 8659.000\n",
      "epoch 9, loss 0.0048, train acc 0.887, test acc 8659.000\n",
      "epoch 10, loss 0.0047, train acc 0.889, test acc 8670.000\n",
      "epoch 11, loss 0.0046, train acc 0.892, test acc 8698.000\n",
      "epoch 12, loss 0.0045, train acc 0.892, test acc 8689.000\n",
      "epoch 13, loss 0.0044, train acc 0.893, test acc 8725.000\n",
      "epoch 14, loss 0.0044, train acc 0.896, test acc 8717.000\n",
      "epoch 15, loss 0.0042, train acc 0.899, test acc 8687.000\n",
      "epoch 16, loss 0.0042, train acc 0.900, test acc 8747.000\n",
      "epoch 17, loss 0.0042, train acc 0.900, test acc 8738.000\n",
      "epoch 18, loss 0.0041, train acc 0.901, test acc 8700.000\n",
      "epoch 19, loss 0.0040, train acc 0.904, test acc 8723.000\n",
      "epoch 20, loss 0.0040, train acc 0.904, test acc 8743.000\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "epoches = 20\n",
    "lr = 0.001\n",
    "trainer = tf.keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def train_model(train_data=None):\n",
    "    for epoch in range(epoches):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0.0\n",
    "        for x, y in train_data:\n",
    "            with tf.GradientTape() as tp:\n",
    "                y_hat = net(x)\n",
    "                loss = cross_entropy(y, y_hat)\n",
    "            grads = tp.gradient(loss, params)\n",
    "            # 更新梯度\n",
    "            trainer.apply_gradients(zip(grads, params))\n",
    "            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=1) == tf.cast(y, dtype=tf.int64), dtype=tf.float32))\n",
    "            train_l_sum += loss\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_db, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "        \n",
    "train_model(train_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简洁实现\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    keras.layers.Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.7130 - accuracy: 0.6599 - val_loss: 0.4597 - val_accuracy: 0.8310\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 37s 40ms/step - loss: 0.4907 - accuracy: 0.8262 - val_loss: 0.4172 - val_accuracy: 0.8508\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.4436 - accuracy: 0.8454 - val_loss: 0.3893 - val_accuracy: 0.8592\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.4171 - accuracy: 0.8533 - val_loss: 0.3802 - val_accuracy: 0.8622\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 39s 42ms/step - loss: 0.3987 - accuracy: 0.8592 - val_loss: 0.3709 - val_accuracy: 0.8651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c77354e2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss = 'sparse_categorical_crossentropy',   # 没有one-hot所以选sparse\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(train_db,epochs=5,validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
